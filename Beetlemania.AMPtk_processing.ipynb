{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM.ITS.barcodes.fasta    BM_ITS_barcodes.txt      BeetleMania_04122017.bam\r\n",
      "BM.LSU.barcodes.fasta    BM_LSU_barcodes.txt      Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "#I've made a fasta file of the barcodes for each amplicon ITS and LSU, this is what is in the folder\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\r\n",
      "Primers hard-coded into AMPtk:\r\n",
      "----------------------------------\r\n",
      "16S_V3       CCTACGGGNGGCWGCAG\r\n",
      "16S_V4       GACTACHVGGGTATCTAATCC\r\n",
      "COI-F        GGTCAACAAATCATAAAGATATTGG\r\n",
      "COI-R        GGWACTAATCAATTTCCAAATCC\r\n",
      "ITS1         TCCGTAGGTGAACCTGCGG\r\n",
      "ITS1-F       CTTGGTCATTTAGAGGAAGTAA\r\n",
      "ITS2         GCTGCGTTCTTCATCGATGC\r\n",
      "ITS3         GCATCGATGAAGAACGCAGC\r\n",
      "ITS3_KYO2    GATGAAGAACGYAGYRAA\r\n",
      "ITS4         TCCTCCGCTTATTGATATGC\r\n",
      "ITS4-B       CAGGAGACTTGTACACGGTCCAG\r\n",
      "JH-LS-369rc  CTTCCCTTTCAACAATTTCAC\r\n",
      "LR0R         ACCCGCTGAACTTAAGC\r\n",
      "LR2R         AAGAACTTTGAAAAGAG\r\n",
      "fITS7        GTGARTCATCGAATCTTTG\r\n",
      "----------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "#I'll then demultiplex, first lets check primers that are hardcoded into AMPtk\n",
    "#you could also specify primer sequences directly, i.e. -f GTGARTCATCGAATCTTTG\n",
    "!amptk primers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[07:52:38 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[07:52:38 AM]\u001b[0m: amptk v.0.9.0, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[07:52:38 AM]\u001b[0m: Converting Ion Torrent BAM file to FASTQ using BedTools\n",
      "\u001b[92m[07:54:54 AM]\u001b[0m: Foward primer: AGTGARTCATCGAATCTTTG,  Rev comp'd rev primer: GCATATCAATAAGCGGAGGA\n",
      "\u001b[92m[07:54:54 AM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[07:55:01 AM]\u001b[0m: 6,122,373 reads (4.7 GB)\n",
      "-------------------------------------------------------\n",
      "\u001b[92m[08:41:36 AM]\u001b[0m: Concatenating Demuxed Files\n",
      "\u001b[92m[08:42:06 AM]\u001b[0m: 6,122,373 total reads\n",
      "\u001b[92m[08:42:06 AM]\u001b[0m: 2,923,103 valid Barcode\n",
      "\u001b[92m[08:42:06 AM]\u001b[0m: 2,618,782 Fwd Primer found, 2,202,811 Rev Primer found\n",
      "\u001b[92m[08:42:06 AM]\u001b[0m: 174,159 discarded too short (< 125 bp)\n",
      "\u001b[92m[08:42:06 AM]\u001b[0m: 2,444,623 valid output reads\n",
      "\u001b[92m[08:42:13 AM]\u001b[0m: Found 46 barcoded samples\n",
      "                Sample:  Count\n",
      "                 BC.73:  96325\n",
      "                 BC.79:  82931\n",
      "                 BC.75:  82834\n",
      "               SynMock:  81122\n",
      "                 BC.87:  80459\n",
      "                 BC.84:  77947\n",
      "                 BC.94:  75056\n",
      "                 BC.74:  74838\n",
      "                 BC.59:  70061\n",
      "                 BC.82:  67999\n",
      "                 BC.96:  66346\n",
      "                 BC.95:  63942\n",
      "                 BC.91:  60337\n",
      "                 BC.90:  57768\n",
      "                 BC.83:  57106\n",
      "                 BC.93:  56657\n",
      "                 BC.88:  56283\n",
      "                 BC.50:  54547\n",
      "                 BC.58:  53698\n",
      "                 BC.76:  52615\n",
      "                 BC.77:  52367\n",
      "                 BC.78:  52290\n",
      "                 BC.53:  51480\n",
      "                 BC.92:  51257\n",
      "                 BC.68:  50587\n",
      "                 BC.61:  50414\n",
      "                 BC.86:  49276\n",
      "                 BC.85:  47766\n",
      "                 BC.71:  46923\n",
      "                 BC.56:  45759\n",
      "                 BC.55:  45689\n",
      "                 BC.57:  44773\n",
      "                 BC.89:  42882\n",
      "                 BC.49:  41090\n",
      "                 BC.63:  40157\n",
      "                 BC.69:  39740\n",
      "                 BC.52:  39054\n",
      "                 BC.64:  39001\n",
      "                 BC.81:  37168\n",
      "                 BC.67:  36806\n",
      "                 BC.60:  35507\n",
      "                 BC.51:  35142\n",
      "                 BC.54:  34784\n",
      "                 BC.80:  32635\n",
      "                 BC.65:  17224\n",
      "                 BC.72:  15981\n",
      "\u001b[92m[08:42:13 AM]\u001b[0m: Output file:  its.demux.fq (1.2 GB)\n",
      "\u001b[92m[08:42:13 AM]\u001b[0m: Mapping file: its.mapping_file.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd: \u001b[0mamptk cluster -i its.demux.fq -o out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We've used fITS7 and ITS4 for generating the ITS2 amplicons, so will demulitplex like this\n",
    "#which will demux barcodes, remove primers, and trim/pad amplicons to 250 bp\n",
    "#shortest known ITS2 sequences are around 150 bp, so we will drop all reads less than 125 to get rid of \n",
    "#primer dimers and other likely non-biological sequences\n",
    "!amptk ion -i BeetleMania_04122017.bam -o its -f fITS7 -r ITS4 --barcode_fasta BM.ITS.barcodes.fasta --min_len 125 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looks like above code ran well and we have a range of ~15,000 to 95,000 sequences from each sample.  Despite trying hard to get this to be equal, it never will be.  Usually we try to have at least 10,000 reads per sample.  You can also see that the script has created a QIIME-like mapping file, which we can edit to add our meta data to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[09:44:22 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[09:44:22 AM]\u001b[0m: amptk v.0.9.0, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[09:44:22 AM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[09:44:24 AM]\u001b[0m: 2,444,623 reads (1.2 GB)\n",
      "\u001b[92m[09:44:24 AM]\u001b[0m: Quality Filtering, expected errors < 1.0\n",
      "\u001b[92m[09:45:14 AM]\u001b[0m: 1,174,205 reads passed\n",
      "\u001b[92m[09:45:14 AM]\u001b[0m: De-replication (remove duplicate reads)\n",
      "\u001b[92m[09:45:17 AM]\u001b[0m: 86,475 reads passed\n",
      "\u001b[92m[09:45:18 AM]\u001b[0m: Clustering OTUs (UPARSE)\n",
      "\u001b[92m[09:45:20 AM]\u001b[0m: 397 OTUs\n",
      "\u001b[92m[09:45:20 AM]\u001b[0m: Cleaning up padding from OTUs\n",
      "\u001b[92m[09:45:20 AM]\u001b[0m: Mapping Reads to OTUs and Building OTU table\n",
      "\u001b[92m[09:49:31 AM]\u001b[0m: 2,036,787 reads mapped to OTUs (83%)\n",
      "-------------------------------------------------------\n",
      "OTU Clustering Script has Finished Successfully\n",
      "-------------------------------------------------------\n",
      "Clustered OTUs: /Users/jon/projects/beetlemania/its.cluster.otus.fa\n",
      "OTU Table: /Users/jon/projects/beetlemania/its.otu_table.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk filter -i its.otu_table.txt -f its.cluster.otus.fa -b <mock barcode>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first lets cluster the data using UPARSE to generate OTUs\n",
    "!amptk cluster -i its.demux.fq -o its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: amptk v.0.9.0, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Loading OTU table: its.otu_table.txt\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: OTU table contains 397 OTUs\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Mapping OTUs to Mock Community (USEARCH)\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Sorting OTU table naturally\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Normalizing OTU table to number of reads per sample\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Index bleed, mock into samples: 14.225539%.  Index bleed, samples into mock: 0.016000%.\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Will use value of 14.300000% for index-bleed OTU filtering.\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Index bleed into samples is abnormally high (14.300000%), if you have biological mock you should use `--calculate in`\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Filtering OTU table down to 389 OTUs\n",
      "\u001b[92m[09:50:00 AM]\u001b[0m: Filtering valid OTUs\n",
      "-------------------------------------------------------\n",
      "OTU Table filtering finished\n",
      "-------------------------------------------------------\n",
      "OTU Table Stats:      its.stats.txt\n",
      "Sorted OTU table:     its.sorted.txt\n",
      "Normalized/filter:    its.normalized.txt\n",
      "Final Binary table:   its.final.binary.txt\n",
      "Final OTU table:      its.final.txt\n",
      "Filtered OTUs:        its.filtered.otus.fa\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk taxonomy -f its.filtered.otus.fa -i its.final.txt -m mapping_file.txt -d ITS2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#since we have a mock in this run, specifically synthetic mock, we can filter dataset\n",
    "!amptk filter -i its.otu_table.txt -f its.cluster.otus.fa -b SynMock -m synmock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: amptk v.0.9.0, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Loading OTU table: its.otu_table.txt\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: OTU table contains 397 OTUs\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Mapping OTUs to Mock Community (USEARCH)\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Sorting OTU table naturally\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Normalizing OTU table to number of reads per sample\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Index bleed, samples into mock: 0.016000%.\n",
      "\u001b[92m[09:55:57 AM]\u001b[0m: Will use value of 0.100000% for index-bleed OTU filtering.\n",
      "\u001b[92m[09:55:58 AM]\u001b[0m: Filtering OTU table down to 391 OTUs\n",
      "\u001b[92m[09:55:58 AM]\u001b[0m: Filtering valid OTUs\n",
      "-------------------------------------------------------\n",
      "OTU Table filtering finished\n",
      "-------------------------------------------------------\n",
      "OTU Table Stats:      its.stats.txt\n",
      "Sorted OTU table:     its.sorted.txt\n",
      "Normalized/filter:    its.normalized.txt\n",
      "Final Binary table:   its.final.binary.txt\n",
      "Final OTU table:      its.final.txt\n",
      "Filtered OTUs:        its.filtered.otus.fa\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk taxonomy -f its.filtered.otus.fa -i its.final.txt -m mapping_file.txt -d ITS2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this result is indicative of contamination, not index-bleed. Index-bleed on Ion happens ~ 0.2%.\n",
    "#While we used the synthetic mock, we will modify for right now\n",
    "!amptk filter -i its.otu_table.txt -f its.cluster.otus.fa -b SynMock -m synmock --calculate in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#because of the contamination from the synthetic mock into a few of the other samples, one could either drop those \n",
    "#samples or we could just drop those OTUs, I'll just quickly drop those OTUs for the time being\n",
    "!grep -v '^mock' its.final.txt > its.final.clean.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SampleID\tBarcodeSequence\tLinkerPrimerSequence\tReversePrimer\tphinchID\tTreatment\r\n",
      "BC.49\tTCCTAACATAAC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTCCTAACATAACAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.49\tno_data\r\n",
      "BC.50\tCGGACAATGGC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCGGACAATGGCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.50\tno_data\r\n",
      "BC.51\tTTGAGCCTATTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTTGAGCCTATTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.51\tno_data\r\n",
      "BC.52\tCCGCATGGAAC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCCGCATGGAACAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.52\tno_data\r\n",
      "BC.53\tCTGGCAATCCTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCTGGCAATCCTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.53\tno_data\r\n",
      "BC.54\tCCGGAGAATCGC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCCGGAGAATCGCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.54\tno_data\r\n",
      "BC.55\tTCCACCTCCTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTCCACCTCCTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.55\tno_data\r\n",
      "BC.56\tCAGCATTAATTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCAGCATTAATTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.56\tno_data\r\n",
      "BC.57\tTCTGGCAACGGC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTCTGGCAACGGCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tBC.57\tno_data\r\n"
     ]
    }
   ],
   "source": [
    "#how lets have a look at the mapping file that the script made for us\n",
    "!head its.mapping_file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open this file in excel or text editor to add your meta data, can be any number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SampleID\tBarcodeSequence\tLinkerPrimerSequence\tReversePrimer\tphinchID\tTreatment\tcage\tsection\r\n",
      "BC.49\tTCCTAACATAAC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTCCTAACATAACAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_1_T\tT0\t1\tT\r\n",
      "BC.50\tCGGACAATGGC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCGGACAATGGCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_1_M\tT0\t1\tM\r\n",
      "BC.51\tTTGAGCCTATTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTTGAGCCTATTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_1_B\tT0\t1\tB\r\n",
      "BC.52\tCCGCATGGAAC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCCGCATGGAACAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_2_T\tT0\t2\tT\r\n",
      "BC.53\tCTGGCAATCCTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCTGGCAATCCTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_2_M\tT0\t2\tM\r\n",
      "BC.54\tCCGGAGAATCGC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCCGGAGAATCGCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_2_B\tT0\t2\tB\r\n",
      "BC.55\tTCCACCTCCTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTCCACCTCCTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_3_T\tT0\t3\tT\r\n",
      "BC.56\tCAGCATTAATTC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGCAGCATTAATTCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_3_M\tT0\t3\tM\r\n",
      "BC.57\tTCTGGCAACGGC\tCCATCTCATCCCTGCGTGTCTCCGACTCAGTCTGGCAACGGCAGTGARTCATCGAATCTTTG\tTCCTCCGCTTATTGATATGC\tT0_3_B\tT0\t3\tB\r\n"
     ]
    }
   ],
   "source": [
    "#added meta data, lets see what that looks like, i but the time T0 and T1 into Treatment section\n",
    "!head its.meta.mapping_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[03:11:10 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[03:11:10 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[03:11:10 PM]\u001b[0m: Loading FASTA Records\n",
      "\u001b[92m[03:11:10 PM]\u001b[0m: 391 OTUs\n",
      "\u001b[92m[03:11:10 PM]\u001b[0m: Global alignment OTUs with usearch_global (USEARCH)\n",
      "\u001b[92m[03:11:15 PM]\u001b[0m: Classifying OTUs with UTAX (USEARCH)\n",
      "\u001b[92m[03:11:15 PM]\u001b[0m: Classifying OTUs with SINTAX (USEARCH)\n",
      "\u001b[92m[03:12:39 PM]\u001b[0m: Appending taxonomy to OTU table and OTUs\n",
      "\u001b[92m[03:12:39 PM]\u001b[0m: Generating phylogenetic tree\n",
      "\u001b[92m[03:12:40 PM]\u001b[0m: Taxonomy finished: its.taxonomy.txt\n",
      "\u001b[92m[03:12:40 PM]\u001b[0m: Classic OTU table with taxonomy: its.otu_table.taxonomy.txt\n",
      "\u001b[92m[03:12:41 PM]\u001b[0m: BIOM OTU table created: its.biom\n",
      "\u001b[92m[03:12:41 PM]\u001b[0m: OTUs with taxonomy: its.otus.taxonomy.fa\n",
      "\u001b[92m[03:12:41 PM]\u001b[0m: OTU phylogeny: its.tree.phy\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#now that we have filtered OTU table, filtered OTUs, and a meta mapping file, we will run taxonomy\n",
    "!amptk taxonomy -f its.filtered.otus.fa -i its.final.clean.txt -m its.meta.mapping_file.txt -d ITS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">OTU1 EF445342;k:Fungi,p:Ascomycota,c:Dothideomycetes,o:Botryosphaeriales,f:Botryosphaeriaceae,g:Sphaeropsis,s:Sphaeropsis sapinea\r\n",
      "AACGCACATTGCGCCCCTTGGCATTCCGAGGGGCATGCCTGTTCGAGCGTCATTACAACC\r\n",
      "CTCAAGCTCTGCTTGGTATTGGGCGCCGTCCTCTCTGCGGACGCGCCTTAAAGACCTCGG\r\n",
      "CGGTGGCTGTTCAGCCCTCAAGCGTAGTAGAATACACCTCGCTTTGGAGCGGTTGGCGTC\r\n",
      "GCCCGCCGGACGAACCTTCTGAACTTTTCTCAAGGTTGACCTCGGATCAGGTAGGGATAC\r\n",
      "CCGCTGAACT\r\n",
      ">OTU2 FJ627241;k:Fungi,p:Basidiomycota,c:Agaricomycetes,o:Russulales,f:Bondarzewiaceae,g:Heterobasidion\r\n",
      "AACGCACCTTGCGCCCTTTGGTATTCCGAAGGGCACGCCTGTTTGAGTGTCGTGAAATTC\r\n",
      "TCAACCCCGTGCTTTTCTTGTGAAAGCGCGTGGGCTTGGACTTGGAGGTTTTGCTGGTCC\r\n",
      "TCGCGGATCGGCTCCTCTCAAATGCATTAGCGAGACCCTTGTGGTGCCGTCCCCGGTGTG\r\n"
     ]
    }
   ],
   "source": [
    "#Okay now we have taxonomy assigned to our ITS data, we can see that by looking at first few records\n",
    "!head its.otus.taxonomy.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now onto LSU data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[08:43:00 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[08:43:00 AM]\u001b[0m: amptk v.0.9.0, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[08:43:00 AM]\u001b[0m: Foward primer: AACCCGCTGAACTTAAGC,  Rev comp'd rev primer: GTGAAATTGTTGAAAGGGAAG\n",
      "\u001b[92m[08:43:00 AM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[08:43:07 AM]\u001b[0m: 6,122,373 reads (4.7 GB)\n",
      "-------------------------------------------------------\n",
      "\u001b[92m[09:32:39 AM]\u001b[0m: Concatenating Demuxed Files\n",
      "\u001b[92m[09:33:03 AM]\u001b[0m: 6,122,373 total reads\n",
      "\u001b[92m[09:33:03 AM]\u001b[0m: 2,360,851 valid Barcode\n",
      "\u001b[92m[09:33:03 AM]\u001b[0m: 2,269,140 Fwd Primer found, 1,577,089 Rev Primer found\n",
      "\u001b[92m[09:33:03 AM]\u001b[0m: 4,195 discarded too short (< 200 bp)\n",
      "\u001b[92m[09:33:03 AM]\u001b[0m: 1,572,894 valid output reads\n",
      "\u001b[92m[09:33:09 AM]\u001b[0m: Found 49 barcoded samples\n",
      "                Sample:  Count\n",
      "                 BC.34:  42490\n",
      "                 BC.36:  41083\n",
      "                 BC.46:  39659\n",
      "                 BC.30:  39500\n",
      "                 BC.48:  38524\n",
      "                  BC.9:  37994\n",
      "                 BC.23:  37223\n",
      "                 BC.43:  37198\n",
      "                 BC.66:  37191\n",
      "                  BC.7:  36869\n",
      "                 BC.37:  36599\n",
      "                 BC.11:  36453\n",
      "                 BC.10:  36172\n",
      "                 BC.19:  35983\n",
      "                 BC.14:  35159\n",
      "                 BC.39:  35159\n",
      "                 BC.18:  35005\n",
      "                 BC.24:  34758\n",
      "                 BC.20:  34322\n",
      "                 BC.40:  33948\n",
      "                 BC.28:  33807\n",
      "                  BC.3:  33784\n",
      "                 BC.45:  33711\n",
      "                 BC.27:  33553\n",
      "                 BC.41:  33529\n",
      "                 BC.22:  33230\n",
      "                 BC.47:  33193\n",
      "                  BC.8:  33014\n",
      "                 BC.16:  33008\n",
      "                 BC.32:  32964\n",
      "                 BC.26:  32742\n",
      "                  BC.2:  32283\n",
      "                 BC.17:  32144\n",
      "                 BC.42:  32064\n",
      "                 BC.15:  31957\n",
      "                  BC.5:  31863\n",
      "                 BC.12:  31538\n",
      "                  BC.4:  30391\n",
      "                 BC.25:  30110\n",
      "                 BC.44:  28498\n",
      "                  BC.6:  27968\n",
      "                 BC.21:  26513\n",
      "                 BC.38:  25882\n",
      "                  BC.1:  23913\n",
      "                 BC.35:  22669\n",
      "                 BC.29:  21374\n",
      "                 BC.33:  20831\n",
      "                 BC.31:  14561\n",
      "                 BC.13:  511\n",
      "\u001b[92m[09:33:09 AM]\u001b[0m: Output file:  lsu.demux.fq (1.1 GB)\n",
      "\u001b[92m[09:33:09 AM]\u001b[0m: Mapping file: lsu.mapping_file.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd: \u001b[0mamptk cluster -i lsu.demux.fq -o out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#since I'm less sure of the size for LSU, I'll run first batch of only keeping full length reads\n",
    "#I will re-use the fastq file from ITS run, note it was just converted from BAM to FASTQ, so save a few minutes doing\n",
    "#the conversion again.  LSU should have less length variation than ITS, perhaps full length will work well\n",
    "#if not, we will run again at a shorter length\n",
    "!amptk ion -i its.fastq -o lsu -f LR0R -r JH-LS-369rc --barcode_fasta BM.LSU.barcodes.fasta --min_len 200 --full_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Found 49 barcoded samples\n",
      "              Sample:  Count\n",
      "               BC.34:  42490\n",
      "               BC.36:  41083\n",
      "               BC.46:  39659\n",
      "               BC.30:  39500\n",
      "               BC.48:  38524\n",
      "                BC.9:  37994\n",
      "               BC.23:  37223\n",
      "               BC.43:  37198\n",
      "               BC.66:  37191\n",
      "                BC.7:  36869\n",
      "               BC.37:  36599\n",
      "               BC.11:  36453\n",
      "               BC.10:  36172\n",
      "               BC.19:  35983\n",
      "               BC.14:  35159\n",
      "               BC.39:  35159\n",
      "               BC.18:  35005\n",
      "               BC.24:  34758\n",
      "               BC.20:  34322\n",
      "               BC.40:  33948\n",
      "               BC.28:  33807\n",
      "                BC.3:  33784\n",
      "               BC.45:  33711\n",
      "               BC.27:  33553\n",
      "               BC.41:  33529\n",
      "               BC.22:  33230\n",
      "               BC.47:  33193\n",
      "                BC.8:  33014\n",
      "               BC.16:  33008\n",
      "               BC.32:  32964\n",
      "               BC.26:  32742\n",
      "                BC.2:  32283\n",
      "               BC.17:  32144\n",
      "               BC.42:  32064\n",
      "               BC.15:  31957\n",
      "                BC.5:  31863\n",
      "               BC.12:  31538\n",
      "                BC.4:  30391\n",
      "               BC.25:  30110\n",
      "               BC.44:  28498\n",
      "                BC.6:  27968\n",
      "               BC.21:  26513\n",
      "               BC.38:  25882\n",
      "                BC.1:  23913\n",
      "               BC.35:  22669\n",
      "               BC.29:  21374\n",
      "               BC.33:  20831\n",
      "               BC.31:  14561\n",
      "               BC.13:  511\n",
      "----------------------------------\n",
      "Read count: 1572894\n",
      "Read length average: 396\n",
      "Read length range: 201 - 577 bp\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#lets see what the read length distribution is in this LSU sample\n",
    "!amptk show -i lsu.demux.fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1 samples\r\n",
      "Kept 1572383 reads out of 1572894 total reads\r\n"
     ]
    }
   ],
   "source": [
    "#we can see from here that the average length is ~ 396 bp, and we have some suspiciously long sequences as well. \n",
    "#remember that we used 400 bp sequencing kit, which can get out to about 450 bp, so for right now I will leave those\n",
    "#those sample in.  I would also drop BC.13 here as the number of reads is too low, so lets do that now like this\n",
    "!amptk remove -i lsu.demux.fq -l BC.13 -o lsu.clean.fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[03:34:29 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[03:34:29 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[03:34:29 PM]\u001b[0m: R v3.3.3; DADA2 v1.3.3\n",
      "\u001b[92m[03:34:29 PM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[03:34:31 PM]\u001b[0m: 1,572,383 reads (1.1 GB)\n",
      "\u001b[92m[03:34:46 PM]\u001b[0m: Quality Filtering, expected errors < 2.0\n",
      "\u001b[92m[03:34:46 PM]\u001b[0m: 11,463 reads passed\n",
      "\u001b[92m[03:34:46 PM]\u001b[0m: Splitting FASTQ file by Sample into individual files\n",
      "\u001b[92m[03:34:47 PM]\u001b[0m: Running DADA2 pipeline\n",
      "\u001b[92m[03:36:09 PM]\u001b[0m: 90 total inferred sequences (iSeqs)\n",
      "\u001b[92m[03:36:09 PM]\u001b[0m: 1 denovo chimeras removed\n",
      "\u001b[92m[03:36:09 PM]\u001b[0m: 89 valid iSeqs\n",
      "\u001b[92m[03:36:09 PM]\u001b[0m: Mapping reads to DADA2 iSeqs\n",
      "\u001b[92m[03:40:48 PM]\u001b[0m: 1,420,504 reads mapped to iSeqs (90%)\n",
      "\u001b[92m[03:40:48 PM]\u001b[0m: Clustering iSeqs at 97% to generate biological OTUs\n",
      "\u001b[92m[03:40:48 PM]\u001b[0m: 76 OTUs generated\n",
      "\u001b[92m[03:40:48 PM]\u001b[0m: Mapping reads to OTUs\n",
      "\u001b[92m[03:45:12 PM]\u001b[0m: 1,404,335 reads mapped to OTUs (89%)\n",
      "-------------------------------------------------------\n",
      "DADA2 Script has Finished Successfully\n",
      "-------------------------------------------------------\n",
      "Inferred iSeqs: /Users/jon/projects/beetlemania/lsu.iSeqs.fa\n",
      "iSeq OTU Table: /Users/jon/projects/beetlemania/lsu.otu_table.txt\n",
      "Clustered OTUs: /Users/jon/projects/beetlemania/lsu.cluster.otus.fa\n",
      "OTU Table: /Users/jon/projects/beetlemania/lsu.cluster.otu_table.txt\n",
      "iSeqs 2 OTUs: /Users/jon/projects/beetlemania/lsu.iSeqs2clusters.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk filter -i lsu.cluster.otu_table.txt -f lsu.cluster.otus.fa -b <mock barcode>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#because there is less variation in LSU region, I'll try to run DADA2 denoising which can differentiate between single\n",
    "#base pair differences. because the read length is longer, I'm increasing the maxEE quality filter value\n",
    "!amptk dada2 -i lsu.clean.fq -o lsu -e 2.0 --pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[03:57:43 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[03:57:43 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[03:57:43 PM]\u001b[0m: Foward primer: AACCCGCTGAACTTAAGC,  Rev comp'd rev primer: GTGAAATTGTTGAAAGGGAAG\n",
      "\u001b[92m[03:57:43 PM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[03:57:48 PM]\u001b[0m: 6,122,373 reads (4.7 GB)\n",
      "-------------------------------------------------------\n",
      "\u001b[92m[04:46:24 PM]\u001b[0m: Concatenating Demuxed Files\n",
      "\u001b[92m[04:46:56 PM]\u001b[0m: 6,122,373 total reads\n",
      "\u001b[92m[04:46:56 PM]\u001b[0m: 2,360,851 valid Barcode\n",
      "\u001b[92m[04:46:56 PM]\u001b[0m: 2,269,140 Fwd Primer found, 1,577,089 Rev Primer found\n",
      "\u001b[92m[04:46:56 PM]\u001b[0m: 49,907 discarded too short (< 250 bp)\n",
      "\u001b[92m[04:46:56 PM]\u001b[0m: 2,219,233 valid output reads\n",
      "\u001b[92m[04:47:04 PM]\u001b[0m: Found 49 barcoded samples\n",
      "                Sample:  Count\n",
      "                 BC.30:  62570\n",
      "                 BC.46:  54578\n",
      "                 BC.19:  54354\n",
      "                 BC.45:  54222\n",
      "                  BC.5:  53679\n",
      "                 BC.48:  53023\n",
      "                 BC.40:  53015\n",
      "                 BC.42:  52947\n",
      "                 BC.28:  52714\n",
      "                 BC.47:  52387\n",
      "                 BC.34:  52104\n",
      "                 BC.32:  52046\n",
      "                 BC.41:  51808\n",
      "                 BC.10:  51344\n",
      "                 BC.27:  50736\n",
      "                 BC.39:  50468\n",
      "                 BC.66:  49828\n",
      "                  BC.2:  49305\n",
      "                 BC.36:  48882\n",
      "                  BC.4:  48643\n",
      "                  BC.9:  48289\n",
      "                 BC.43:  48079\n",
      "                 BC.24:  47555\n",
      "                  BC.7:  47133\n",
      "                  BC.3:  46869\n",
      "                 BC.11:  46088\n",
      "                 BC.20:  45258\n",
      "                 BC.37:  45129\n",
      "                 BC.12:  43702\n",
      "                 BC.26:  43513\n",
      "                 BC.18:  43267\n",
      "                  BC.8:  43201\n",
      "                 BC.15:  43042\n",
      "                 BC.25:  43031\n",
      "                 BC.17:  42768\n",
      "                 BC.22:  42427\n",
      "                 BC.14:  42147\n",
      "                 BC.23:  42119\n",
      "                 BC.21:  41128\n",
      "                 BC.35:  41017\n",
      "                 BC.16:  40644\n",
      "                 BC.38:  40412\n",
      "                 BC.33:  40240\n",
      "                 BC.29:  36439\n",
      "                 BC.44:  35188\n",
      "                  BC.6:  34848\n",
      "                  BC.1:  32480\n",
      "                 BC.31:  23940\n",
      "                 BC.13:  627\n",
      "\u001b[92m[04:47:04 PM]\u001b[0m: Output file:  lsu2.demux.fq (1.5 GB)\n",
      "\u001b[92m[04:47:04 PM]\u001b[0m: Mapping file: lsu2.mapping_file.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd: \u001b[0mamptk cluster -i lsu2.demux.fq -o out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we see that probably too few reads have passed the quality filter.  However, you can also see that 90% of the reads\n",
    "#mapped to those iSeqs, so this might be okay.  But because of the long length of the reads using full length \n",
    "#we are likely getting a lot of low quality basepairs near the 3' end of the reads. Also there were some really long reads\n",
    "#So I'm going to go back and re-do the demuxing and do a trim/pad\n",
    "#length of 350 bp, so we can get rid of those longer reads which might be problematic but will also keep reads where \n",
    "# we did not find the reverse primer, trimming to 350 seems reasonable.\n",
    "!amptk ion -i its.fastq -o lsu2 -f LR0R -r JH-LS-369rc --barcode_fasta BM.LSU.barcodes.fasta --min_len 250 -l 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1 samples\r\n",
      "Kept 2218606 reads out of 2219233 total reads\r\n"
     ]
    }
   ],
   "source": [
    "#clean up, remove BC.13\n",
    "!amptk remove -i lsu2.demux.fq -l BC.13 -o lsu2.clean.fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[04:51:15 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[04:51:15 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[04:51:16 PM]\u001b[0m: R v3.3.3; DADA2 v1.3.3\n",
      "\u001b[92m[04:51:16 PM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[04:51:17 PM]\u001b[0m: 2,218,606 reads (1.5 GB)\n",
      "\u001b[92m[04:51:36 PM]\u001b[0m: Quality Filtering, expected errors < 2.0\n",
      "\u001b[92m[04:51:37 PM]\u001b[0m: 15,809 reads passed\n",
      "\u001b[92m[04:51:37 PM]\u001b[0m: Splitting FASTQ file by Sample into individual files\n",
      "\u001b[92m[04:51:38 PM]\u001b[0m: Running DADA2 pipeline\n",
      "\u001b[92m[04:53:37 PM]\u001b[0m: 118 total inferred sequences (iSeqs)\n",
      "\u001b[92m[04:53:37 PM]\u001b[0m: 1 denovo chimeras removed\n",
      "\u001b[92m[04:53:37 PM]\u001b[0m: 117 valid iSeqs\n",
      "\u001b[92m[04:53:37 PM]\u001b[0m: Mapping reads to DADA2 iSeqs\n",
      "\u001b[92m[05:26:41 PM]\u001b[0m: 1,818,479 reads mapped to iSeqs (82%)\n",
      "\u001b[92m[05:26:41 PM]\u001b[0m: Clustering iSeqs at 97% to generate biological OTUs\n",
      "\u001b[92m[05:26:41 PM]\u001b[0m: 94 OTUs generated\n",
      "\u001b[92m[05:26:41 PM]\u001b[0m: Mapping reads to OTUs\n",
      "\u001b[92m[05:33:18 PM]\u001b[0m: 1,789,716 reads mapped to OTUs (81%)\n",
      "-------------------------------------------------------\n",
      "DADA2 Script has Finished Successfully\n",
      "-------------------------------------------------------\n",
      "Inferred iSeqs: /Users/jon/projects/beetlemania/lsu2.iSeqs.fa\n",
      "iSeq OTU Table: /Users/jon/projects/beetlemania/lsu2.otu_table.txt\n",
      "Clustered OTUs: /Users/jon/projects/beetlemania/lsu2.cluster.otus.fa\n",
      "OTU Table: /Users/jon/projects/beetlemania/lsu2.cluster.otu_table.txt\n",
      "iSeqs 2 OTUs: /Users/jon/projects/beetlemania/lsu2.iSeqs2clusters.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk filter -i lsu2.cluster.otu_table.txt -f lsu2.cluster.otus.fa -b <mock barcode>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run dada2 again\n",
    "!amptk dada2 -i lsu2.clean.fq -o lsu2 -e 2.0 --pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Loading OTU table: lsu2.otu_table.txt\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: OTU table contains 117 OTUs\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Sorting OTU table naturally\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Normalizing OTU table to number of reads per sample\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Overwriting auto detect index-bleed, setting to 0.100000%\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Filtering OTU table down to 117 OTUs\n",
      "\u001b[92m[05:40:35 PM]\u001b[0m: Filtering valid OTUs\n",
      "-------------------------------------------------------\n",
      "OTU Table filtering finished\n",
      "-------------------------------------------------------\n",
      "OTU Table Stats:      lsu2.stats.txt\n",
      "Sorted OTU table:     lsu2.sorted.txt\n",
      "Normalized/filter:    lsu2.normalized.txt\n",
      "Final Binary table:   lsu2.final.binary.txt\n",
      "Final OTU table:      lsu2.final.txt\n",
      "Filtered OTUs:        lsu2.filtered.otus.fa\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk taxonomy -f lsu2.filtered.otus.fa -i lsu2.final.txt -m mapping_file.txt -d ITS2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we don't have mock community here, but we will still run a filter at 0.1% (same as calculated from ITS)\n",
    "#here we will use the DADA2 inferred sequences first\n",
    "!amptk filter -i lsu2.otu_table.txt -f lsu2.iSeqs.fa -p 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[05:44:43 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[05:44:43 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[05:44:43 PM]\u001b[0m: Loading FASTA Records\n",
      "\u001b[92m[05:44:43 PM]\u001b[0m: 117 OTUs\n",
      "\u001b[92m[05:44:43 PM]\u001b[0m: Global alignment OTUs with usearch_global (USEARCH)\n",
      "\u001b[92m[05:44:44 PM]\u001b[0m: Classifying OTUs with UTAX (USEARCH)\n",
      "\u001b[92m[05:44:44 PM]\u001b[0m: Classifying OTUs with SINTAX (USEARCH)\n",
      "\u001b[92m[05:44:57 PM]\u001b[0m: Appending taxonomy to OTU table and OTUs\n",
      "\u001b[92m[05:44:57 PM]\u001b[0m: Generating phylogenetic tree\n",
      "\u001b[92m[05:44:57 PM]\u001b[0m: Taxonomy finished: lsu2.taxonomy.txt\n",
      "\u001b[92m[05:44:57 PM]\u001b[0m: Classic OTU table with taxonomy: lsu2.otu_table.taxonomy.txt\n",
      "\u001b[92m[05:44:58 PM]\u001b[0m: BIOM OTU table created: lsu2.biom\n",
      "\u001b[92m[05:44:58 PM]\u001b[0m: OTUs with taxonomy: lsu2.otus.taxonomy.fa\n",
      "\u001b[92m[05:44:58 PM]\u001b[0m: OTU phylogeny: lsu2.tree.phy\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#I did same thing for LSU mapping file as for ITS, added Meta Data\n",
    "!amptk taxonomy -f lsu2.filtered.otus.fa -i lsu2.final.txt -m lsu.meta.mapping_file.txt -d LSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[07:24:13 PM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[07:24:13 PM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[07:24:13 PM]\u001b[0m: R v3.3.3; DADA2 v1.3.3\n",
      "\u001b[92m[07:24:13 PM]\u001b[0m: Loading FASTQ Records\n",
      "\u001b[92m[07:24:15 PM]\u001b[0m: 2,218,606 reads (1.5 GB)\n",
      "\u001b[92m[07:24:36 PM]\u001b[0m: Quality Filtering, expected errors < 2.0\n",
      "\u001b[92m[07:25:07 PM]\u001b[0m: 1,207,703 reads passed\n",
      "\u001b[92m[07:25:07 PM]\u001b[0m: Splitting FASTQ file by Sample into individual files\n",
      "\u001b[92m[07:26:27 PM]\u001b[0m: Running DADA2 pipeline\n",
      "\u001b[92m[03:57:07 AM]\u001b[0m: 617 total inferred sequences (iSeqs)\n",
      "\u001b[92m[03:57:07 AM]\u001b[0m: 209 denovo chimeras removed\n",
      "\u001b[92m[03:57:07 AM]\u001b[0m: 408 valid iSeqs\n",
      "\u001b[92m[03:57:07 AM]\u001b[0m: Mapping reads to DADA2 iSeqs\n",
      "\u001b[92m[04:03:52 AM]\u001b[0m: 1,878,655 reads mapped to iSeqs (85%)\n",
      "\u001b[92m[04:03:52 AM]\u001b[0m: Clustering iSeqs at 97% to generate biological OTUs\n",
      "\u001b[92m[04:03:52 AM]\u001b[0m: 197 OTUs generated\n",
      "\u001b[92m[04:03:52 AM]\u001b[0m: Mapping reads to OTUs\n",
      "\u001b[92m[04:10:17 AM]\u001b[0m: 1,843,273 reads mapped to OTUs (83%)\n",
      "-------------------------------------------------------\n",
      "DADA2 Script has Finished Successfully\n",
      "-------------------------------------------------------\n",
      "Inferred iSeqs: /Users/jon/projects/beetlemania/lsu3.iSeqs.fa\n",
      "iSeq OTU Table: /Users/jon/projects/beetlemania/lsu3.otu_table.txt\n",
      "Clustered OTUs: /Users/jon/projects/beetlemania/lsu3.cluster.otus.fa\n",
      "OTU Table: /Users/jon/projects/beetlemania/lsu3.cluster.otu_table.txt\n",
      "iSeqs 2 OTUs: /Users/jon/projects/beetlemania/lsu3.iSeqs2clusters.txt\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk filter -i lsu3.cluster.otu_table.txt -f lsu3.cluster.otus.fa -b <mock barcode>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try to run dada2 again, had this fastq_qmin=2 setting in Q filtering, wondering if that dropped unnecessary reads.\n",
    "#yes it did, so these results should be more robust, it also took 7 hours to run....\n",
    "!amptk dada2 -i lsu2.clean.fq -o lsu3 -e 2.0 --pool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: Loading OTU table: lsu3.otu_table.txt\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: OTU table contains 408 OTUs\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: Sorting OTU table naturally\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: Normalizing OTU table to number of reads per sample\n",
      "\u001b[92m[08:07:27 AM]\u001b[0m: Overwriting auto detect index-bleed, setting to 0.100000%\n",
      "\u001b[92m[08:07:28 AM]\u001b[0m: Filtering OTU table down to 408 OTUs\n",
      "\u001b[92m[08:07:28 AM]\u001b[0m: Filtering valid OTUs\n",
      "-------------------------------------------------------\n",
      "OTU Table filtering finished\n",
      "-------------------------------------------------------\n",
      "OTU Table Stats:      lsu3.stats.txt\n",
      "Sorted OTU table:     lsu3.sorted.txt\n",
      "Normalized/filter:    lsu3.normalized.txt\n",
      "Final Binary table:   lsu3.final.binary.txt\n",
      "Final OTU table:      lsu3.final.txt\n",
      "Filtered OTUs:        lsu3.filtered.otus.fa\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk taxonomy -f lsu3.filtered.otus.fa -i lsu3.final.txt -m mapping_file.txt -d ITS2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now filter the iSeqs at 0.1%\n",
    "!amptk filter -i lsu3.otu_table.txt -f lsu3.iSeqs.fa -p 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[08:08:04 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[08:08:04 AM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[08:08:04 AM]\u001b[0m: Loading FASTA Records\n",
      "\u001b[92m[08:08:04 AM]\u001b[0m: 408 OTUs\n",
      "\u001b[92m[08:08:04 AM]\u001b[0m: Global alignment OTUs with usearch_global (USEARCH)\n",
      "\u001b[92m[08:08:06 AM]\u001b[0m: Classifying OTUs with UTAX (USEARCH)\n",
      "\u001b[92m[08:08:06 AM]\u001b[0m: Classifying OTUs with SINTAX (USEARCH)\n",
      "\u001b[92m[08:08:27 AM]\u001b[0m: Appending taxonomy to OTU table and OTUs\n",
      "\u001b[92m[08:08:27 AM]\u001b[0m: Generating phylogenetic tree\n",
      "\u001b[92m[08:08:28 AM]\u001b[0m: Taxonomy finished: lsu3.taxonomy.txt\n",
      "\u001b[92m[08:08:28 AM]\u001b[0m: Classic OTU table with taxonomy: lsu3.otu_table.taxonomy.txt\n",
      "\u001b[92m[08:08:29 AM]\u001b[0m: BIOM OTU table created: lsu3.biom\n",
      "\u001b[92m[08:08:29 AM]\u001b[0m: OTUs with taxonomy: lsu3.otus.taxonomy.fa\n",
      "\u001b[92m[08:08:29 AM]\u001b[0m: OTU phylogeny: lsu3.tree.phy\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!amptk taxonomy -f lsu3.filtered.otus.fa -i lsu3.final.txt -m lsu.meta.mapping_file.txt -d LSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: Loading OTU table: lsu3.cluster.otu_table.txt\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: OTU table contains 197 OTUs\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: Sorting OTU table naturally\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: Normalizing OTU table to number of reads per sample\n",
      "\u001b[92m[08:13:28 AM]\u001b[0m: Overwriting auto detect index-bleed, setting to 0.100000%\n",
      "\u001b[92m[08:13:29 AM]\u001b[0m: Filtering OTU table down to 197 OTUs\n",
      "\u001b[92m[08:13:29 AM]\u001b[0m: Filtering valid OTUs\n",
      "-------------------------------------------------------\n",
      "OTU Table filtering finished\n",
      "-------------------------------------------------------\n",
      "OTU Table Stats:      lsu3.cluster.stats.txt\n",
      "Sorted OTU table:     lsu3.cluster.sorted.txt\n",
      "Normalized/filter:    lsu3.cluster.normalized.txt\n",
      "Final Binary table:   lsu3.cluster.final.binary.txt\n",
      "Final OTU table:      lsu3.cluster.final.txt\n",
      "Filtered OTUs:        lsu3.cluster.filtered.otus.fa\n",
      "-------------------------------------------------------\n",
      "\u001b[93m\n",
      "Example of next cmd:\u001b[0m amptk taxonomy -f lsu3.cluster.filtered.otus.fa -i lsu3.cluster.final.txt -m mapping_file.txt -d ITS2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#also filter the clustered dada2 output to see what it looks like\n",
    "!amptk filter -i lsu3.cluster.otu_table.txt -o lsu3.cluster -f lsu3.cluster.otus.fa -p 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\u001b[92m[08:13:55 AM]\u001b[0m: OS: MacOSX 10.12.4, 8 cores, ~ 16 GB RAM. Python: 2.7.12\n",
      "\u001b[92m[08:13:55 AM]\u001b[0m: amptk v.0.9.1, USEARCH v9.2.64, VSEARCH v2.4.2\n",
      "\u001b[92m[08:13:55 AM]\u001b[0m: Loading FASTA Records\n",
      "\u001b[92m[08:13:55 AM]\u001b[0m: 197 OTUs\n",
      "\u001b[92m[08:13:55 AM]\u001b[0m: Global alignment OTUs with usearch_global (USEARCH)\n",
      "\u001b[92m[08:13:56 AM]\u001b[0m: Classifying OTUs with UTAX (USEARCH)\n",
      "\u001b[92m[08:13:56 AM]\u001b[0m: Classifying OTUs with SINTAX (USEARCH)\n",
      "\u001b[92m[08:14:14 AM]\u001b[0m: Appending taxonomy to OTU table and OTUs\n",
      "\u001b[92m[08:14:14 AM]\u001b[0m: Generating phylogenetic tree\n",
      "\u001b[92m[08:14:14 AM]\u001b[0m: Taxonomy finished: lsu3.cluster.taxonomy.txt\n",
      "\u001b[92m[08:14:14 AM]\u001b[0m: Classic OTU table with taxonomy: lsu3.cluster.otu_table.taxonomy.txt\n",
      "\u001b[92m[08:14:15 AM]\u001b[0m: BIOM OTU table created: lsu3.cluster.biom\n",
      "\u001b[92m[08:14:15 AM]\u001b[0m: OTUs with taxonomy: lsu3.cluster.otus.taxonomy.fa\n",
      "\u001b[92m[08:14:15 AM]\u001b[0m: OTU phylogeny: lsu3.cluster.tree.phy\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!amptk taxonomy -f lsu3.cluster.filtered.otus.fa -i lsu3.cluster.final.txt -m lsu.meta.mapping_file.txt -d LSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
